{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8588840,"sourceType":"datasetVersion","datasetId":5137255}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploring AI & ML Job Trends in the U.S.\n\n# Notebook Version: v1  \n**Focus**: Dataset loading and basic structural preview  \n\nThis notebook is part of a versioned project exploring trends in AI/ML job postings in the U.S.  \nThis version focuses on loading the dataset, checking its structure, and identifying surface-level issues.\n","metadata":{}},{"cell_type":"code","source":"#importing the necessary libraries\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T09:53:54.631894Z","iopub.execute_input":"2025-07-31T09:53:54.632265Z","iopub.status.idle":"2025-07-31T09:53:54.638184Z","shell.execute_reply.started":"2025-07-31T09:53:54.632241Z","shell.execute_reply":"2025-07-31T09:53:54.637331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Overview\n\n- Source: Kaggle ‚Äì AI and ML Job Listings USA  \n- File path: `/kaggle/input/ai-and-ml-job-listings-usa/ai_ml_jobs_linkedin.csv","metadata":{}},{"cell_type":"markdown","source":"## Load and Preview Data\n\nLoading the dataset into a DataFrame and preview the structure to understand its basic layout.\n","metadata":{}},{"cell_type":"code","source":"# Load the dataset\nus_jobs_df = pd.read_csv('/kaggle/input/ai-and-ml-job-listings-usa/ai_ml_jobs_linkedin.csv')\n\n# Create a working copy\njobs_df = us_jobs_df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T09:53:58.089341Z","iopub.execute_input":"2025-07-31T09:53:58.089649Z","iopub.status.idle":"2025-07-31T09:53:58.220849Z","shell.execute_reply.started":"2025-07-31T09:53:58.089625Z","shell.execute_reply":"2025-07-31T09:53:58.220096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preview first 2 rows\njobs_df.head(2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T07:47:13.624483Z","iopub.execute_input":"2025-07-31T07:47:13.624795Z","iopub.status.idle":"2025-07-31T07:47:13.657465Z","shell.execute_reply.started":"2025-07-31T07:47:13.624774Z","shell.execute_reply":"2025-07-31T07:47:13.656625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check dataset shape\nprint(f\"Rows: {jobs_df.shape[0]}, Columns: {jobs_df.shape[1]}\")\n\n# Data types and non-null info\njobs_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T07:47:46.957273Z","iopub.execute_input":"2025-07-31T07:47:46.957549Z","iopub.status.idle":"2025-07-31T07:47:46.981957Z","shell.execute_reply.started":"2025-07-31T07:47:46.957530Z","shell.execute_reply":"2025-07-31T07:47:46.980437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary stats for numeric columns\njobs_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:20:17.683470Z","iopub.execute_input":"2025-07-31T12:20:17.683763Z","iopub.status.idle":"2025-07-31T12:20:17.734000Z","shell.execute_reply.started":"2025-07-31T12:20:17.683741Z","shell.execute_reply":"2025-07-31T12:20:17.733143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initial Observations and Notes\n\n- The dataset contains **862 rows** and **10 columns**.\n- Some columns such as `companyName`, `publishedAt`, and `sector` contain missing values.\n- Columns like `applicationsCount` and `publishedAt` may need data type conversions in the next version.\n- No immediate data loading issues were encountered.\n","metadata":{}},{"cell_type":"markdown","source":"# Notebook Version: v2  \n**Focus**: Data Cleaning and Formatting  \n \nThis version focuses on cleaning the dataset, handling missing values, renaming columns, correcting data types, and preparing the data for analysis.\n","metadata":{}},{"cell_type":"code","source":"#previewing the data again\njobs_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:40:55.451973Z","iopub.execute_input":"2025-07-31T10:40:55.452255Z","iopub.status.idle":"2025-07-31T10:40:55.463285Z","shell.execute_reply.started":"2025-07-31T10:40:55.452236Z","shell.execute_reply":"2025-07-31T10:40:55.462282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling missing/null values","metadata":{}},{"cell_type":"code","source":"#checking for null values if any\njobs_df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T08:56:12.210438Z","iopub.execute_input":"2025-07-31T08:56:12.210732Z","iopub.status.idle":"2025-07-31T08:56:12.219246Z","shell.execute_reply.started":"2025-07-31T08:56:12.210711Z","shell.execute_reply":"2025-07-31T08:56:12.217938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Type Fix","metadata":{}},{"cell_type":"code","source":"#handling null values\n\n#filling the null values in columns 'companyName' and 'experienceLevel' as 'Unknown'\njobs_df[['companyName','sector']] = jobs_df[['companyName','sector']].fillna('Unknown')\n\n#handling the null value for column 'publishedAt' using ffill() assuming that the post has been updated nearly at that date\njobs_df['publishedAt'] = jobs_df['publishedAt'].fillna(method='ffill')\n\n#check if null value still exists\njobs_df.isna().sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T09:57:18.038399Z","iopub.execute_input":"2025-07-31T09:57:18.038680Z","iopub.status.idle":"2025-07-31T09:57:18.051223Z","shell.execute_reply.started":"2025-07-31T09:57:18.038659Z","shell.execute_reply":"2025-07-31T09:57:18.050416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NOTE:\nFilled publishedAt using forward fill to maintain temporal continuity, assuming listings are updated close to previous records.","metadata":{}},{"cell_type":"code","source":"#checking for the datatypes \njobs_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:03:41.971120Z","iopub.execute_input":"2025-07-31T10:03:41.971443Z","iopub.status.idle":"2025-07-31T10:03:41.991130Z","shell.execute_reply.started":"2025-07-31T10:03:41.971420Z","shell.execute_reply":"2025-07-31T10:03:41.990316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# converting 'publishedAt' into datetime data type  \njobs_df['publishedAt'] = pd.to_datetime(jobs_df['publishedAt'])\n\n# converting 'applicationsCount' into integer data type\n#first we need to extract the count of the applications \njobs_df['applicationsCount'] = jobs_df['applicationsCount'].str.extract(r'(\\d+)')[0]\n\n#now convert the 'applicationsCount' dtype to numeric\njobs_df['applicationsCount'] = pd.to_numeric(jobs_df['applicationsCount'])\njobs_df.info()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:20:55.072564Z","iopub.execute_input":"2025-07-31T10:20:55.072864Z","iopub.status.idle":"2025-07-31T10:20:55.084708Z","shell.execute_reply.started":"2025-07-31T10:20:55.072842Z","shell.execute_reply":"2025-07-31T10:20:55.083689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Removing the columns that are not useful for my analysis","metadata":{}},{"cell_type":"code","source":"#making a new df to store only the columns that are useful for my analysis\n\nupdated_jobs = jobs_df.drop(columns=['description','sector','workType'])\nupdated_jobs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T11:10:44.673851Z","iopub.execute_input":"2025-07-31T11:10:44.674127Z","iopub.status.idle":"2025-07-31T11:10:44.688554Z","shell.execute_reply.started":"2025-07-31T11:10:44.674107Z","shell.execute_reply":"2025-07-31T11:10:44.687467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Duplicate Check and Removal","metadata":{}},{"cell_type":"code","source":"# checking the duplicate values that exists (based on all columns)\n# updated_jobs.duplicated().sum()\nupdated_jobs[updated_jobs.duplicated()]\n\n# dropping the duplicated values\nupdated_jobs.drop_duplicates(inplace=True)\n\n# check if any row exist that have same title,companyName, location and publishedAt\nupdated_jobs.duplicated(subset=['title', 'companyName', 'location', 'publishedAt']).sum()\n\n# removing the dupliacted values\nduplicate_vals = updated_jobs.duplicated(subset=['title', 'companyName', 'location', 'publishedAt'])\nupdated_jobs = updated_jobs[~duplicate_vals].copy()\n\n#resetting the index after droppingt the duplicate values\nupdated_jobs.reset_index(drop=True,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T11:36:59.833327Z","iopub.execute_input":"2025-07-31T11:36:59.833652Z","iopub.status.idle":"2025-07-31T11:36:59.845330Z","shell.execute_reply.started":"2025-07-31T11:36:59.833629Z","shell.execute_reply":"2025-07-31T11:36:59.844436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Whitespace Stripping \n\n","metadata":{}},{"cell_type":"code","source":"#stripping the whitespaces if any, from the string based columns\n\nfor col in ['title','companyName','location','experienceLevel','contractType']:\n    updated_jobs[col] = updated_jobs[col].str.strip()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:01:03.848012Z","iopub.execute_input":"2025-07-31T12:01:03.848317Z","iopub.status.idle":"2025-07-31T12:01:03.868257Z","shell.execute_reply.started":"2025-07-31T12:01:03.848293Z","shell.execute_reply":"2025-07-31T12:01:03.867412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Category Cleaning","metadata":{}},{"cell_type":"code","source":"#1. title\n# for consistency I am converting the titles into title case\nupdated_jobs['title'] = updated_jobs['title'].str.title()\n\n#check if the function is applied properly\nupdated_jobs['title'].head(3)\n\n#2. location\n# I will be splitting the location column into two parts: one is for city and other is for state\nlocation_split = updated_jobs['location'].str.split(',',n=1,expand=True)\n\n#adding the city column\nupdated_jobs['city'] = location_split[0].str.strip()\n\n#adding the state column\nupdated_jobs['state'] = location_split[1].str.strip()\n\n#check if any null value has been added due to the above two columns\nupdated_jobs.isna().sum()\n\n#handle the null values\nupdated_jobs['state'] = updated_jobs['state'].fillna('Unknown')\n\n#rechecking for null values\nupdated_jobs.isna().sum()\n\n#removing the location column as it is no more useful\nupdated_jobs.drop('location',axis='columns',inplace=True)\n\n#3. publishedAt\n# I will be asplitting this column also into two parts year and month (day is not useful)\n\nupdated_jobs['year'] = updated_jobs['publishedAt'].dt.year\nupdated_jobs['month'] = updated_jobs['publishedAt'].dt.month\n\n#dropping the publishedAt column because it is not useful\nupdated_jobs.drop('publishedAt',axis='columns',inplace=True)\nupdated_jobs.columns\n\n#4. companyName\n# converting the company's name into title case so that it remains consistent throughout\nupdated_jobs['companyName'] = updated_jobs['companyName'].str.title()\n\n#check if the change has been made properly\nupdated_jobs['companyName'].head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:11:33.889453Z","iopub.execute_input":"2025-07-31T12:11:33.890057Z","iopub.status.idle":"2025-07-31T12:11:33.904619Z","shell.execute_reply.started":"2025-07-31T12:11:33.890027Z","shell.execute_reply":"2025-07-31T12:11:33.903929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Cleaning and Structuring Summary\n\nIn this version, I focused on cleaning and structuring the dataset to prepare it for meaningful analysis. The original dataset had multiple inconsistencies and mixed-format fields which could hinder exploration and insights.\n\n## üöÄ Key actions\n- Selected 7 relevant columns for the analysis.\n- Cleaned categorical columns (`title`, `companyName`, `location`) for consistency.\n- Split complex fields like `location` and `publishedAt` into simpler, analyzable components (city, state, year, month).\n- Handled missing values in `state` by filling with \"Unknown\".\n\n## ‚ö†Ô∏è Challenges\n- Some job titles were overly specific or inconsistent (e.g., different casing, role modifiers). I resolved this with title casing but might need more grouping later.\n- The `location` field didn‚Äôt follow a uniform format in all rows ‚Äî some were missing state info, which led to NaN values after splitting.\n- The `publishedAt` field contained full timestamps, which were not useful at this stage. It took care to isolate only the useful components (year/month) without losing meaning.\n\n## üéØ Learnings\n- Even basic string cleaning and formatting (like `.str.title()` or `.str.strip()`) can greatly improve consistency in the dataset.\n- Breaking down complex columns (like `location` and `publishedAt`) can make future analysis smoother and more insightful.\n- It's important to analyze columns one by one instead of applying generic cleaning ‚Äî each column may need unique handling.\n\n","metadata":{}},{"cell_type":"markdown","source":"## ","metadata":{}}]}